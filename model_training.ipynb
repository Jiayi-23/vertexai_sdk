{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: mlflow>=2.0 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: ipykernel>=6.12 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (6.26.0)\n",
      "Requirement already satisfied: ipython>=7.32 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (8.12.3)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (2.0.3)\n",
      "Requirement already satisfied: scikit-learn>=0.24.0 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: pyspark>=3.5.0 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.5.0)\n",
      "Requirement already satisfied: pandas_gbq>=0.19.2 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (0.19.2)\n",
      "Requirement already satisfied: lightgbm>=4.1.0 in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (4.1.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /home/user/.local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.36.4)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (8.1.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.8/dist-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (6.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.12.0 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (4.25.0)\n",
      "Requirement already satisfied: scipy<2 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (1.10.1)\n",
      "Requirement already satisfied: querystring-parser<2 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: packaging<24 in /usr/local/lib/python3.8/dist-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (23.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: Flask<4 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: numpy<2 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (1.24.4)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (3.5.1)\n",
      "Requirement already satisfied: pytz<2024 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: matplotlib<4 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (3.7.3)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (2.0.22)\n",
      "Requirement already satisfied: psutil<6 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (5.9.6)\n",
      "Requirement already satisfied: cloudpickle<3 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: docker<7,>=4.0.0 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (6.1.3)\n",
      "Requirement already satisfied: gunicorn<22 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (21.2.0)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<7,>=3.7.0 in /usr/local/lib/python3.8/dist-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (6.6.0)\n",
      "Requirement already satisfied: databricks-cli<1,>=0.8.7 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (0.18.0)\n",
      "Requirement already satisfied: pyarrow<14,>=4.0.0 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (13.0.0)\n",
      "Requirement already satisfied: entrypoints<1 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=2.1.0 in /usr/local/lib/python3.8/dist-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (3.1.31)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (1.12.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (0.4.4)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /home/user/.local/lib/python3.8/site-packages (from mlflow>=2.0->-r requirements.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (6.3.3)\n",
      "Requirement already satisfied: nest-asyncio in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (1.5.8)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (5.13.0)\n",
      "Requirement already satisfied: pyzmq>=20 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (25.1.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (0.1.4)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (5.5.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (0.1.6)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/user/.local/lib/python3.8/site-packages (from ipykernel>=6.12->-r requirements.txt (line 2)) (8.5.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (0.19.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (2.16.1)\n",
      "Requirement already satisfied: backcall in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (0.6.3)\n",
      "Requirement already satisfied: pickleshare in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.8/dist-packages (from ipython>=7.32->-r requirements.txt (line 3)) (4.8.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from ipython>=7.32->-r requirements.txt (line 3)) (4.6.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (3.0.39)\n",
      "Requirement already satisfied: decorator in /home/user/.local/lib/python3.8/site-packages (from ipython>=7.32->-r requirements.txt (line 3)) (5.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/user/.local/lib/python3.8/site-packages (from pandas>=1.0.5->-r requirements.txt (line 4)) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/.local/lib/python3.8/site-packages (from pandas>=1.0.5->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/user/.local/lib/python3.8/site-packages (from scikit-learn>=0.24.0->-r requirements.txt (line 5)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/.local/lib/python3.8/site-packages (from scikit-learn>=0.24.0->-r requirements.txt (line 5)) (3.2.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/user/.local/lib/python3.8/site-packages (from pyspark>=3.5.0->-r requirements.txt (line 7)) (0.10.9.7)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3.0.0dev,>=2.16.2 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (2.22.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (3.13.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0dev,>=2.10.2 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (2.12.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (56.2.0)\n",
      "Requirement already satisfied: google-auth>=2.13.0 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (2.23.4)\n",
      "Requirement already satisfied: pydata-google-auth>=1.5.0 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.8.2)\n",
      "Requirement already satisfied: db-dtypes<2.0.0,>=1.0.4 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib>=0.7.0 in /home/user/.local/lib/python3.8/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.1.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 10)) (1.22.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 10)) (2.13.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 10)) (1.10.4)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /home/user/.local/lib/python3.8/site-packages (from google-cloud-aiplatform->-r requirements.txt (line 10)) (2.0.2)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from alembic!=1.10.0,<2->mlflow>=2.0->-r requirements.txt (line 1)) (5.12.0)\n",
      "Requirement already satisfied: Mako in /home/user/.local/lib/python3.8/site-packages (from alembic!=1.10.0,<2->mlflow>=2.0->-r requirements.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: pyjwt>=1.7.0 in /home/user/.local/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.0->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /home/user/.local/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.0->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.7 in /usr/local/lib/python3.8/dist-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.0->-r requirements.txt (line 1)) (1.26.16)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in /home/user/.local/lib/python3.8/site-packages (from databricks-cli<1,>=0.8.7->mlflow>=2.0->-r requirements.txt (line 1)) (3.2.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/user/.local/lib/python3.8/site-packages (from docker<7,>=4.0.0->mlflow>=2.0->-r requirements.txt (line 1)) (1.6.4)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/user/.local/lib/python3.8/site-packages (from Flask<4->mlflow>=2.0->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/user/.local/lib/python3.8/site-packages (from Flask<4->mlflow>=2.0->-r requirements.txt (line 1)) (1.7.0)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/user/.local/lib/python3.8/site-packages (from Flask<4->mlflow>=2.0->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from gitpython<4,>=2.1.0->mlflow>=2.0->-r requirements.txt (line 1)) (4.0.10)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/user/.local/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.61.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/user/.local/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.59.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/user/.local/lib/python3.8/site-packages (from google-api-core<3.0.0dev,>=2.10.2->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.59.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/user/.local/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (5.3.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/user/.local/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/.local/lib/python3.8/site-packages (from google-auth>=2.13.0->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (0.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/.local/lib/python3.8/site-packages (from google-auth-oauthlib>=0.7.0->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (1.3.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (2.6.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (2.3.3)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform->-r requirements.txt (line 10)) (0.12.7)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/user/.local/lib/python3.8/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform->-r requirements.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata!=4.7.0,<7,>=3.7.0->mlflow>=2.0->-r requirements.txt (line 1)) (3.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/user/.local/lib/python3.8/site-packages (from jedi>=0.16->ipython>=7.32->-r requirements.txt (line 3)) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/.local/lib/python3.8/site-packages (from Jinja2<4,>=2.11->mlflow>=2.0->-r requirements.txt (line 1)) (2.1.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=6.12->-r requirements.txt (line 2)) (2.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (3.1.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (4.43.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (0.12.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/user/.local/lib/python3.8/site-packages (from matplotlib<4->mlflow>=2.0->-r requirements.txt (line 1)) (10.1.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect>4.3->ipython>=7.32->-r requirements.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/user/.local/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=7.32->-r requirements.txt (line 3)) (0.2.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/.local/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow>=2.0->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.17.3->mlflow>=2.0->-r requirements.txt (line 1)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.17.3->mlflow>=2.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/user/.local/lib/python3.8/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.0->-r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/user/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.32->-r requirements.txt (line 3)) (2.0.1)\n",
      "Requirement already satisfied: pure-eval in /home/user/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.32->-r requirements.txt (line 3)) (0.2.2)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/user/.local/lib/python3.8/site-packages (from stack-data->ipython>=7.32->-r requirements.txt (line 3)) (2.4.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow>=2.0->-r requirements.txt (line 1)) (5.0.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/user/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.13.0->pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (0.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ws_verAI_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ws_verAI_training.py\n",
    "# Databricks notebook source\n",
    "from all_imports import *\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#set variables\n",
    "# Open the file and load the file\n",
    "with open('variables_setting.yaml') as f:\n",
    "    variables = yaml.load(f, Loader=SafeLoader)\n",
    "    print(variables)\n",
    "\n",
    "globals().update(variables)\n",
    "\n",
    "\n",
    "#os.environ[\"GCLOUD_PROJECT\"] = \"apmena-sandbox-dna-apac-dv\"\n",
    "\n",
    "\n",
    "client = bigquery.Client() \n",
    "sql = \"\"\"\n",
    "select *\n",
    "from apmena-sandbox-dna-apac-dv.dbx_poc.sample_records\n",
    "\"\"\"\n",
    "df = client.query(sql).to_dataframe()\n",
    "\n",
    "df.head()\n",
    "\n",
    "split_date = df.sellout_date.max() + relativedelta(months=-3)\n",
    "df_target = df[df['sellout_date'] > split_date]\n",
    "df_target = df_target[[cid, sales]].groupby(cid).sum().reset_index()\n",
    "df_target.columns = [cid, 'target_sales']\n",
    "df_target['target_sales'] = df_target['target_sales'].apply(pd.to_numeric)\n",
    "\n",
    "df_train = df[df['sellout_date'] <= split_date]\n",
    "\n",
    "df_consumer = df[['consumer_code','tourist_flag', 'birthyear', 'gender', 'is_email_valid', 'is_address_valid', 'is_mobile_valid','is_instagram_contactable']].drop_duplicates() \n",
    "client = bigquery.Client() \n",
    "product_sql = \"\"\"\n",
    "select *\n",
    "from apmena-sandbox-dna-apac-dv.dbx_poc_eu.compass_product\n",
    "\"\"\"\n",
    "df_product = client.query(product_sql).to_dataframe()\n",
    "\n",
    "df_product.head()\n",
    "merged_df = pd.merge(df_train, df_product, on='sap_product_code', how='left')\n",
    "\n",
    "#get the unique consumer code to start building feature table\n",
    "df_train = merged_df[[cid]].drop_duplicates()\n",
    "df_train.shape\n",
    "df_train = FE_pipeline.first_last_order(df_train, merged_df, cid, sellout_date)\n",
    "\n",
    "merged_df['year_month'] = merged_df['sellout_date'].dt.to_period('M').astype(str)\n",
    "\n",
    "df_train = FE_pipeline.Flag_purchasing_times_by_T(df_train,merged_df, Time_G, cid, sales)\n",
    "\n",
    "df_train = FE_pipeline.get_lag_sales(df_train, merged_df, sellout_date, month_list, cid, sales)\n",
    "\n",
    "df_train = FE_pipeline.past_xx_month_xx_features(df_train, merged_df, df_product, cid, sellout_date, sales, feature_dic, month_num = 24, column_name = 'signature_code')\n",
    "\n",
    "df_train = FE_pipeline.feature_engineering_order_info(df_train, merged_df, cid, sellout_date, units, order_number, month_num = 12)\n",
    "df_1 = FE_pipeline.get_recency(df_train, merged_df, cid, sellout_date, recency_target_cols)\n",
    "\n",
    "df_cn = consumer_trans.consumer_trans(df_consumer)\n",
    "\n",
    "df_2 = df_1.merge(df_cn, 'left', on = cid)\n",
    "print(df_2.shape)\n",
    "#training dataset post processing\n",
    "df_2 = df_2[df_2['age']<100]\n",
    "#change column types\n",
    "object_cols = df_2.select_dtypes(include = 'object').columns.drop(['consumer_code','the_signature_code_with_the_maximum_of_sellout_gross_value_w_tax_p24m']).tolist()\n",
    "float_cols = df_2.select_dtypes(include = 'float').columns.tolist()\n",
    "print(object_cols)\n",
    "print(float_cols)\n",
    "for i in float_cols:\n",
    "    df_2[i] = df_2[i].fillna(0)\n",
    "    \n",
    "df_2.columns = df_2.columns.str.replace(' ', '_')\n",
    "df_2 = df_2.drop('is_instagram_contactable', axis=1)\n",
    "df_2['consumer_id'] = df_2['consumer_code']\n",
    "\n",
    "\n",
    "\n",
    "new_names = {\n",
    "    \"sum_of_sellout_gross_value_w_tax_of_H._Rubinstein_p24m\":\"sum_of_sales_of_HR_p24m\",\n",
    "    \"sum_of_sellout_gross_value_w_tax_of_Kiehl's_p24m\":\"sum_of_sale_of_KS_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Biotherm_p24m':\"sum_of_sale_of_Bio_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Yves_Saint_Laurent_p24m':\"sum_of_sale_of_YSL_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Lancome_p24m':\"sum_of_sale_of_LC_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Armani_p24m':\"sum_of_sale_of_Armani_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Shu_Uemura_p24m':\"sum_of_sale_of_SU_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Atelier_Cologne_p24m':\"sum_of_sale_of_AC_p24m\",\n",
    "    'sum_of_sellout_gross_value_w_tax_of_Urban_Decay_p24m':\"sum_of_sale_of_UD_p24m\",\n",
    "    'the_signature_code_with_the_maximum_of_sellout_gross_value_w_tax_p24m':'fav_brand_p24m',\n",
    "    'max_of_sellout_gross_value_w_tax_by_signature_code_p24m':'maxsales_by_brand_p24m',\n",
    "    \"H._Rubinstein_recency\": \"HR_recency\",\n",
    "    \"Kiehl's_recency\":'KS_recency'\n",
    "}\n",
    "\n",
    "\n",
    "df_2 = df_2.rename(columns=new_names)\n",
    "df_2.columns = df_2.columns.str.lower()\n",
    "df_2.columns\n",
    "training_df = pd.merge(df_2, df_target, on= 'consumer_code', how = 'left')\n",
    "training_df['target_sales'] = training_df['target_sales'].fillna(0)\n",
    "\n",
    "#writing data to bigquery\n",
    "try:\n",
    "    training_df.to_gbq(destination_table='dbx_poc.training_df', project_id= 'apmena-sandbox-dna-apac-dv')\n",
    "except:\n",
    "    skip\n",
    "\n",
    "y = training_df['target_sales']\n",
    "X = training_df.drop(['consumer_id','consumer_code','target_sales','first_order_date','last_order_date','fav_brand_p24m'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = lgb.LGBMRegressor(objective = 'tweedie', learning_rate=0.1, max_depth=4, reg_lambda=0.5, importance_type = 'gain').fit(X_train, y_train)\n",
    "\n",
    "#write model to gcs bucket\n",
    "import joblib\n",
    "joblib.dump(model, 'lgb_bpp_kr_Luxe231130.pkl')\n",
    "\n",
    "#get evaluation metrics\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_true = y_train.to_numpy()\n",
    "train_MAE, train_MSE, train_MAPE, train_WAPE, train_total_gap, train_spearman = em.print_regressin_metrics(y_train_true, y_train_pred)\n",
    "\n",
    "y_test_true= y_test.to_numpy()\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_MAE, test_MSE, test_MAPE, test_WAPE, test_total_gap, test_spearman = em.print_regressin_metrics(y_test_true, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM python:3.9\n",
    "WORKDIR /JIAYI-TEST\n",
    "COPY requirements.txt .\n",
    "COPY all_imports.py .\n",
    "COPY variables_setting.yaml .\n",
    "COPY steps ./steps\n",
    "\n",
    "# Install dependencies with specified versions\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY ws_verAI_training.py ./ws_verAI_training.py\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"ws_verAI_training.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cloudbuild.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile cloudbuild.yaml\n",
    "\n",
    "steps:\n",
    "# Build the container image\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['build', '-t', 'gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training', '.']\n",
    "# Push the container image to Container Registry\n",
    "- name: 'gcr.io/cloud-builders/docker'\n",
    "  args: ['push', 'gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training']\n",
    "images:\n",
    "- gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 26 file(s) totalling 361.1 KiB before compression.\n",
      "Uploading tarball of [.] to [gs://apmena-sandbox-dna-apac-dv_cloudbuild/source/1701349806.283838-e74eb5bf216743c4a8a4e1165a76676e.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/apmena-sandbox-dna-apac-dv/locations/global/builds/015a857a-065d-46a3-bb6e-60758ef4144b].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/015a857a-065d-46a3-bb6e-60758ef4144b?project=1022207864171 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"015a857a-065d-46a3-bb6e-60758ef4144b\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://apmena-sandbox-dna-apac-dv_cloudbuild/source/1701349806.283838-e74eb5bf216743c4a8a4e1165a76676e.tgz#1701349807398744\n",
      "Copying gs://apmena-sandbox-dna-apac-dv_cloudbuild/source/1701349806.283838-e74eb5bf216743c4a8a4e1165a76676e.tgz#1701349807398744...\n",
      "/ [1 files][ 61.9 KiB/ 61.9 KiB]                                                \n",
      "Operation completed over 1 objects/61.9 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  391.2kB\n",
      "Step #0: Step 1/9 : FROM python:3.9\n",
      "Step #0: 3.9: Pulling from library/python\n",
      "Step #0: 90e5e7d8b87a: Pulling fs layer\n",
      "Step #0: 27e1a8ca91d3: Pulling fs layer\n",
      "Step #0: d3a767d1d12e: Pulling fs layer\n",
      "Step #0: 711be5dc5044: Pulling fs layer\n",
      "Step #0: 7ad48fee4003: Pulling fs layer\n",
      "Step #0: 00ba6b570755: Pulling fs layer\n",
      "Step #0: b7551a8d91b5: Pulling fs layer\n",
      "Step #0: f36f46aa93f3: Pulling fs layer\n",
      "Step #0: 711be5dc5044: Waiting\n",
      "Step #0: 7ad48fee4003: Waiting\n",
      "Step #0: 00ba6b570755: Waiting\n",
      "Step #0: b7551a8d91b5: Waiting\n",
      "Step #0: f36f46aa93f3: Waiting\n",
      "Step #0: 27e1a8ca91d3: Verifying Checksum\n",
      "Step #0: 27e1a8ca91d3: Download complete\n",
      "Step #0: 90e5e7d8b87a: Verifying Checksum\n",
      "Step #0: 90e5e7d8b87a: Download complete\n",
      "Step #0: d3a767d1d12e: Verifying Checksum\n",
      "Step #0: d3a767d1d12e: Download complete\n",
      "Step #0: 7ad48fee4003: Verifying Checksum\n",
      "Step #0: 7ad48fee4003: Download complete\n",
      "Step #0: 00ba6b570755: Verifying Checksum\n",
      "Step #0: 00ba6b570755: Download complete\n",
      "Step #0: b7551a8d91b5: Verifying Checksum\n",
      "Step #0: b7551a8d91b5: Download complete\n",
      "Step #0: f36f46aa93f3: Verifying Checksum\n",
      "Step #0: f36f46aa93f3: Download complete\n",
      "Step #0: 711be5dc5044: Verifying Checksum\n",
      "Step #0: 711be5dc5044: Download complete\n",
      "Step #0: 90e5e7d8b87a: Pull complete\n",
      "Step #0: 27e1a8ca91d3: Pull complete\n",
      "Step #0: d3a767d1d12e: Pull complete\n",
      "Step #0: 711be5dc5044: Pull complete\n",
      "Step #0: 7ad48fee4003: Pull complete\n",
      "Step #0: 00ba6b570755: Pull complete\n",
      "Step #0: b7551a8d91b5: Pull complete\n",
      "Step #0: f36f46aa93f3: Pull complete\n",
      "Step #0: Digest: sha256:f2f14d6dabcbc113512050906474d44cdb2a1139fa26d9b05bef0eedb36ac94a\n",
      "Step #0: Status: Downloaded newer image for python:3.9\n",
      "Step #0:  ---> e9047a20be92\n",
      "Step #0: Step 2/9 : WORKDIR /JIAYI-TEST\n",
      "Step #0:  ---> Running in 6151720b8237\n",
      "Step #0: Removing intermediate container 6151720b8237\n",
      "Step #0:  ---> fc712af8ad65\n",
      "Step #0: Step 3/9 : COPY requirements.txt .\n",
      "Step #0:  ---> f5d0f632b27b\n",
      "Step #0: Step 4/9 : COPY all_imports.py .\n",
      "Step #0:  ---> fbf5e3dc7430\n",
      "Step #0: Step 5/9 : COPY variables_setting.yaml .\n",
      "Step #0:  ---> 24ee2dc0a965\n",
      "Step #0: Step 6/9 : COPY steps ./steps\n",
      "Step #0:  ---> ede888ed5ee2\n",
      "Step #0: Step 7/9 : RUN pip install --no-cache-dir -r requirements.txt\n",
      "Step #0:  ---> Running in c5950823935c\n",
      "Step #0: Collecting mlflow>=2.0\n",
      "Step #0:   Downloading mlflow-2.8.1-py3-none-any.whl (19.0 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.0/19.0 MB 207.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting ipykernel>=6.12\n",
      "Step #0:   Downloading ipykernel-6.27.1-py3-none-any.whl (114 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.6/114.6 kB 192.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting ipython>=7.32\n",
      "Step #0:   Downloading ipython-8.18.1-py3-none-any.whl (808 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 808.2/808.2 kB 233.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting pandas>=1.0.5\n",
      "Step #0:   Downloading pandas-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 202.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting scikit-learn>=0.24.0\n",
      "Step #0:   Downloading scikit_learn-1.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 197.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyspark>=3.5.0\n",
      "Step #0:   Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 316.9/316.9 MB 188.9 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Collecting pandas_gbq>=0.19.2\n",
      "Step #0:   Downloading pandas_gbq-0.19.2-py2.py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting lightgbm>=4.1.0\n",
      "Step #0:   Downloading lightgbm-4.1.0-py3-none-manylinux_2_28_x86_64.whl (3.1 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 237.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-aiplatform\n",
      "Step #0:   Downloading google_cloud_aiplatform-1.36.4-py2.py3-none-any.whl (3.3 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 213.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting markdown<4,>=3.3\n",
      "Step #0:   Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 102.2/102.2 kB 188.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting sqlalchemy<3,>=1.4.0\n",
      "Step #0:   Downloading SQLAlchemy-2.0.23-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 221.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting pytz<2024\n",
      "Step #0:   Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.5/502.5 kB 222.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting sqlparse<1,>=0.4.0\n",
      "Step #0:   Downloading sqlparse-0.4.4-py3-none-any.whl (41 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.2/41.2 kB 129.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting Flask<4\n",
      "Step #0:   Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.7/99.7 kB 187.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting click<9,>=7.0\n",
      "Step #0:   Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 158.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting entrypoints<1\n",
      "Step #0:   Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Step #0: Collecting gitpython<4,>=2.1.0\n",
      "Step #0:   Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 206.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting querystring-parser<2\n",
      "Step #0:   Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Step #0: Collecting Jinja2<4,>=2.11\n",
      "Step #0:   Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 192.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting alembic!=1.10.0,<2\n",
      "Step #0:   Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.8/226.8 kB 212.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting scipy<2\n",
      "Step #0:   Downloading scipy-1.11.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.6/36.6 MB 166.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyyaml<7,>=5.1\n",
      "Step #0:   Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 738.9/738.9 kB 194.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting docker<7,>=4.0.0\n",
      "Step #0:   Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.1/148.1 kB 180.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests<3,>=2.17.3\n",
      "Step #0:   Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 133.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyarrow<15,>=4.0.0\n",
      "Step #0:   Downloading pyarrow-14.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.0/38.0 MB 180.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting packaging<24\n",
      "Step #0:   Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 123.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting databricks-cli<1,>=0.8.7\n",
      "Step #0:   Downloading databricks_cli-0.18.0-py2.py3-none-any.whl (150 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 150.3/150.3 kB 128.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting gunicorn<22\n",
      "Step #0:   Downloading gunicorn-21.2.0-py3-none-any.whl (80 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.2/80.2 kB 169.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting matplotlib<4\n",
      "Step #0:   Downloading matplotlib-3.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.6/11.6 MB 199.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting protobuf<5,>=3.12.0\n",
      "Step #0:   Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 228.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting numpy<2\n",
      "Step #0:   Downloading numpy-1.26.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 204.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting cloudpickle<3\n",
      "Step #0:   Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting importlib-metadata!=4.7.0,<7,>=3.7.0\n",
      "Step #0:   Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Step #0: Collecting debugpy>=1.6.5\n",
      "Step #0:   Downloading debugpy-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.4/3.4 MB 222.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting comm>=0.1.1\n",
      "Step #0:   Downloading comm-0.2.0-py3-none-any.whl (7.0 kB)\n",
      "Step #0: Collecting pyzmq>=20\n",
      "Step #0:   Downloading pyzmq-25.1.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 228.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting tornado>=6.1\n",
      "Step #0:   Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.4/435.4 kB 227.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting traitlets>=5.4.0\n",
      "Step #0:   Downloading traitlets-5.14.0-py3-none-any.whl (85 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.2/85.2 kB 190.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting nest-asyncio\n",
      "Step #0:   Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
      "Step #0: Collecting matplotlib-inline>=0.1\n",
      "Step #0:   Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Step #0: Collecting psutil\n",
      "Step #0:   Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.6/283.6 kB 203.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting jupyter-client>=6.1.12\n",
      "Step #0:   Downloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.9/105.9 kB 179.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting jupyter-core!=5.0.*,>=4.12\n",
      "Step #0:   Downloading jupyter_core-5.5.0-py3-none-any.whl (28 kB)\n",
      "Step #0: Collecting jedi>=0.16\n",
      "Step #0:   Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 223.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting typing-extensions\n",
      "Step #0:   Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Step #0: Collecting pygments>=2.4.0\n",
      "Step #0:   Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 232.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting exceptiongroup\n",
      "Step #0:   Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Step #0: Collecting decorator\n",
      "Step #0:   Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Step #0: Collecting prompt-toolkit<3.1.0,>=3.0.41\n",
      "Step #0:   Downloading prompt_toolkit-3.0.41-py3-none-any.whl (385 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 385.5/385.5 kB 237.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting pexpect>4.3\n",
      "Step #0:   Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.8/63.8 kB 172.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting stack-data\n",
      "Step #0:   Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Step #0: Collecting python-dateutil>=2.8.2\n",
      "Step #0:   Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 195.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting tzdata>=2022.1\n",
      "Step #0:   Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 208.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting threadpoolctl>=2.0.0\n",
      "Step #0:   Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting joblib>=1.1.1\n",
      "Step #0:   Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.2/302.2 kB 180.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting py4j==0.10.9.7\n",
      "Step #0:   Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.5/200.5 kB 92.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting pydata-google-auth>=1.5.0\n",
      "Step #0:   Downloading pydata_google_auth-1.8.2-py2.py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting google-api-core<3.0.0dev,>=2.10.2\n",
      "Step #0:   Downloading google_api_core-2.14.0-py3-none-any.whl (122 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.2/122.2 kB 190.9 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pandas_gbq>=0.19.2->-r requirements.txt (line 8)) (58.1.0)\n",
      "Step #0: Collecting db-dtypes<2.0.0,>=1.0.4\n",
      "Step #0:   Downloading db_dtypes-1.1.1-py2.py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting google-auth>=2.13.0\n",
      "Step #0:   Downloading google_auth-2.23.4-py2.py3-none-any.whl (183 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 183.3/183.3 kB 173.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery!=2.4.*,<4.0.0dev,>=3.3.5\n",
      "Step #0:   Downloading google_cloud_bigquery-3.13.0-py2.py3-none-any.whl (222 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.8/222.8 kB 189.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery-storage<3.0.0dev,>=2.16.2\n",
      "Step #0:   Downloading google_cloud_bigquery_storage-2.23.0-py2.py3-none-any.whl (191 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 191.4/191.4 kB 194.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth-oauthlib>=0.7.0\n",
      "Step #0:   Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Step #0: Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "Step #0:   Downloading google_cloud_storage-2.13.0-py2.py3-none-any.whl (121 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 189.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "Step #0:   Downloading google_cloud_resource_manager-1.10.4-py2.py3-none-any.whl (320 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 321.0/321.0 kB 219.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting shapely<3.0.0dev\n",
      "Step #0:   Downloading shapely-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 136.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0:   Downloading proto_plus-1.22.3-py3-none-any.whl (48 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.1/48.1 kB 134.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting Mako\n",
      "Step #0:   Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 167.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting six>=1.10.0\n",
      "Step #0:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting oauthlib>=3.1.0\n",
      "Step #0:   Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 213.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyjwt>=1.7.0\n",
      "Step #0:   Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Step #0: Collecting tabulate>=0.7.7\n",
      "Step #0:   Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Step #0: Collecting urllib3<3,>=1.26.7\n",
      "Step #0:   Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 104.6/104.6 kB 147.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting websocket-client>=0.32.0\n",
      "Step #0:   Downloading websocket_client-1.6.4-py3-none-any.whl (57 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.3/57.3 kB 141.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting blinker>=1.6.2\n",
      "Step #0:   Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting itsdangerous>=2.1.2\n",
      "Step #0:   Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting Werkzeug>=3.0.0\n",
      "Step #0:   Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 186.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting gitdb<5,>=4.0.1\n",
      "Step #0:   Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 131.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 230.9/230.9 kB 194.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio-1.59.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 195.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.59.3-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting rsa<5,>=3.1.4\n",
      "Step #0:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting pyasn1-modules>=0.2.1\n",
      "Step #0:   Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 193.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting cachetools<6.0,>=2.0.0\n",
      "Step #0:   Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Step #0: Collecting requests-oauthlib>=0.7.0\n",
      "Step #0:   Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Step #0: Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "Step #0:   Downloading google_resumable_media-2.6.0-py2.py3-none-any.whl (80 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.3/80.3 kB 131.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-core<3.0.0dev,>=1.6.0\n",
      "Step #0:   Downloading google_cloud_core-2.3.3-py2.py3-none-any.whl (29 kB)\n",
      "Step #0: Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0:   Downloading grpc_google_iam_v1-0.12.7-py2.py3-none-any.whl (26 kB)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Collecting zipp>=0.5\n",
      "Step #0:   Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Step #0: Collecting parso<0.9.0,>=0.8.3\n",
      "Step #0:   Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.8/100.8 kB 154.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting MarkupSafe>=2.0\n",
      "Step #0:   Downloading MarkupSafe-2.1.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Step #0: Collecting platformdirs>=2.5\n",
      "Step #0:   Downloading platformdirs-4.0.0-py3-none-any.whl (17 kB)\n",
      "Step #0: Collecting pillow>=8\n",
      "Step #0:   Downloading Pillow-10.1.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 226.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting fonttools>=4.22.0\n",
      "Step #0:   Downloading fonttools-4.45.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 213.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting importlib-resources>=3.2.0\n",
      "Step #0:   Downloading importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Step #0: Collecting cycler>=0.10\n",
      "Step #0:   Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Step #0: Collecting pyparsing>=2.3.1\n",
      "Step #0:   Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 103.1/103.1 kB 162.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting kiwisolver>=1.3.1\n",
      "Step #0:   Downloading kiwisolver-1.4.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 165.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting contourpy>=1.0.1\n",
      "Step #0:   Downloading contourpy-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (310 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.0/311.0 kB 208.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting ptyprocess>=0.5\n",
      "Step #0:   Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting wcwidth\n",
      "Step #0:   Downloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting charset-normalizer<4,>=2\n",
      "Step #0:   Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.3/142.3 kB 187.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting idna<4,>=2.5\n",
      "Step #0:   Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 153.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting certifi>=2017.4.17\n",
      "Step #0:   Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 162.5/162.5 kB 202.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting greenlet!=0.4.17\n",
      "Step #0:   Downloading greenlet-3.0.1-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (610 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 610.6/610.6 kB 202.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting pure-eval\n",
      "Step #0:   Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting asttokens>=2.1.0\n",
      "Step #0:   Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Step #0: Collecting executing>=1.2.0\n",
      "Step #0:   Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
      "Step #0: Collecting smmap<6,>=3.0.1\n",
      "Step #0:   Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Step #0: Collecting pyasn1<0.6.0,>=0.4.6\n",
      "Step #0:   Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 157.1 MB/s eta 0:00:00\n",
      "Step #0: Building wheels for collected packages: pyspark\n",
      "Step #0:   Building wheel for pyspark (setup.py): started\n",
      "Step #0:   Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425360 sha256=4fc203dec9fc49db4458a25ba5f5c519c690620b3730d1e94f5933db70f8f737\n",
      "Step #0:   Stored in directory: /tmp/pip-ephem-wheel-cache-am94hqhl/wheels/57/bd/14/ce9e21f2649298678d011fb8f71ed38ee70b42b94fef0be142\n",
      "Step #0: Successfully built pyspark\n",
      "Step #0: Installing collected packages: wcwidth, pytz, py4j, pure-eval, ptyprocess, zipp, websocket-client, urllib3, tzdata, typing-extensions, traitlets, tornado, threadpoolctl, tabulate, sqlparse, smmap, six, pyzmq, pyyaml, pyspark, pyparsing, pyjwt, pygments, pyasn1, psutil, protobuf, prompt-toolkit, platformdirs, pillow, pexpect, parso, packaging, oauthlib, numpy, nest-asyncio, MarkupSafe, kiwisolver, joblib, itsdangerous, idna, grpcio, greenlet, google-crc32c, fonttools, executing, exceptiongroup, entrypoints, decorator, debugpy, cycler, cloudpickle, click, charset-normalizer, certifi, cachetools, blinker, Werkzeug, sqlalchemy, shapely, scipy, rsa, requests, querystring-parser, python-dateutil, pyasn1-modules, pyarrow, proto-plus, matplotlib-inline, Mako, jupyter-core, Jinja2, jedi, importlib-resources, importlib-metadata, gunicorn, googleapis-common-protos, google-resumable-media, gitdb, contourpy, comm, asttokens, stack-data, scikit-learn, requests-oauthlib, pandas, matplotlib, markdown, lightgbm, jupyter-client, grpcio-status, google-auth, gitpython, Flask, docker, databricks-cli, alembic, mlflow, ipython, grpc-google-iam-v1, google-auth-oauthlib, google-api-core, db-dtypes, pydata-google-auth, ipykernel, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery-storage, google-cloud-bigquery, pandas_gbq, google-cloud-aiplatform\n",
      "Step #0: Successfully installed Flask-3.0.0 Jinja2-3.1.2 Mako-1.3.0 MarkupSafe-2.1.3 Werkzeug-3.0.1 alembic-1.12.1 asttokens-2.4.1 blinker-1.7.0 cachetools-5.3.2 certifi-2023.11.17 charset-normalizer-3.3.2 click-8.1.7 cloudpickle-2.2.1 comm-0.2.0 contourpy-1.2.0 cycler-0.12.1 databricks-cli-0.18.0 db-dtypes-1.1.1 debugpy-1.8.0 decorator-5.1.1 docker-6.1.3 entrypoints-0.4 exceptiongroup-1.2.0 executing-2.0.1 fonttools-4.45.1 gitdb-4.0.11 gitpython-3.1.40 google-api-core-2.14.0 google-auth-2.23.4 google-auth-oauthlib-1.1.0 google-cloud-aiplatform-1.36.4 google-cloud-bigquery-3.13.0 google-cloud-bigquery-storage-2.23.0 google-cloud-core-2.3.3 google-cloud-resource-manager-1.10.4 google-cloud-storage-2.13.0 google-crc32c-1.5.0 google-resumable-media-2.6.0 googleapis-common-protos-1.61.0 greenlet-3.0.1 grpc-google-iam-v1-0.12.7 grpcio-1.59.3 grpcio-status-1.59.3 gunicorn-21.2.0 idna-3.6 importlib-metadata-6.8.0 importlib-resources-6.1.1 ipykernel-6.27.1 ipython-8.18.1 itsdangerous-2.1.2 jedi-0.19.1 joblib-1.3.2 jupyter-client-8.6.0 jupyter-core-5.5.0 kiwisolver-1.4.5 lightgbm-4.1.0 markdown-3.5.1 matplotlib-3.8.2 matplotlib-inline-0.1.6 mlflow-2.8.1 nest-asyncio-1.5.8 numpy-1.26.2 oauthlib-3.2.2 packaging-23.2 pandas-2.1.3 pandas_gbq-0.19.2 parso-0.8.3 pexpect-4.9.0 pillow-10.1.0 platformdirs-4.0.0 prompt-toolkit-3.0.41 proto-plus-1.22.3 protobuf-4.25.1 psutil-5.9.6 ptyprocess-0.7.0 pure-eval-0.2.2 py4j-0.10.9.7 pyarrow-14.0.1 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydata-google-auth-1.8.2 pygments-2.17.2 pyjwt-2.8.0 pyparsing-3.1.1 pyspark-3.5.0 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 pyzmq-25.1.1 querystring-parser-1.2.4 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-1.3.2 scipy-1.11.4 shapely-2.0.2 six-1.16.0 smmap-5.0.1 sqlalchemy-2.0.23 sqlparse-0.4.4 stack-data-0.6.3 tabulate-0.9.0 threadpoolctl-3.2.0 tornado-6.4 traitlets-5.14.0 typing-extensions-4.8.0 tzdata-2023.3 urllib3-2.1.0 wcwidth-0.2.12 websocket-client-1.6.4 zipp-3.17.0\n",
      "Step #0: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0: \u001b[0m\u001b[91m\n",
      "Step #0: [notice] A new release of pip is available: 23.0.1 -> 23.3.1\n",
      "Step #0: [notice] To update, run: pip install --upgrade pip\n",
      "Step #0: \u001b[0mRemoving intermediate container c5950823935c\n",
      "Step #0:  ---> c38c84ea68fb\n",
      "Step #0: Step 8/9 : COPY ws_verAI_training.py ./ws_verAI_training.py\n",
      "Step #0:  ---> e82520e8fa70\n",
      "Step #0: Step 9/9 : ENTRYPOINT [\"python3\", \"ws_verAI_training.py\"]\n",
      "Step #0:  ---> Running in 81d20f426950\n",
      "Step #0: Removing intermediate container 81d20f426950\n",
      "Step #0:  ---> 6e7bb36fce20\n",
      "Step #0: Successfully built 6e7bb36fce20\n",
      "Step #0: Successfully tagged gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: Using default tag: latest\n",
      "Step #1: The push refers to repository [gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training]\n",
      "Step #1: 8325e0a7e446: Preparing\n",
      "Step #1: b2925d5d803e: Preparing\n",
      "Step #1: d3c48fa08f3d: Preparing\n",
      "Step #1: 94edcf25cd21: Preparing\n",
      "Step #1: 9f20e5d93682: Preparing\n",
      "Step #1: d5199d512e13: Preparing\n",
      "Step #1: d85d8449b661: Preparing\n",
      "Step #1: 8de1bf907713: Preparing\n",
      "Step #1: 98c78de71776: Preparing\n",
      "Step #1: 180e2073ce74: Preparing\n",
      "Step #1: 2b4cf8a5bd5e: Preparing\n",
      "Step #1: 80bd043d4663: Preparing\n",
      "Step #1: 30f5cd833236: Preparing\n",
      "Step #1: 7c32e0608151: Preparing\n",
      "Step #1: 7cea17427f83: Preparing\n",
      "Step #1: d85d8449b661: Waiting\n",
      "Step #1: 8de1bf907713: Waiting\n",
      "Step #1: 98c78de71776: Waiting\n",
      "Step #1: 180e2073ce74: Waiting\n",
      "Step #1: 2b4cf8a5bd5e: Waiting\n",
      "Step #1: 80bd043d4663: Waiting\n",
      "Step #1: 30f5cd833236: Waiting\n",
      "Step #1: 7c32e0608151: Waiting\n",
      "Step #1: 7cea17427f83: Waiting\n",
      "Step #1: d5199d512e13: Waiting\n",
      "Step #1: 9f20e5d93682: Pushed\n",
      "Step #1: d3c48fa08f3d: Pushed\n",
      "Step #1: 94edcf25cd21: Pushed\n",
      "Step #1: 8de1bf907713: Layer already exists\n",
      "Step #1: 8325e0a7e446: Pushed\n",
      "Step #1: 98c78de71776: Layer already exists\n",
      "Step #1: 180e2073ce74: Layer already exists\n",
      "Step #1: 2b4cf8a5bd5e: Layer already exists\n",
      "Step #1: 80bd043d4663: Layer already exists\n",
      "Step #1: 30f5cd833236: Layer already exists\n",
      "Step #1: 7c32e0608151: Layer already exists\n",
      "Step #1: 7cea17427f83: Layer already exists\n",
      "Step #1: d5199d512e13: Pushed\n",
      "Step #1: d85d8449b661: Pushed\n",
      "Step #1: b2925d5d803e: Pushed\n",
      "Step #1: latest: digest: sha256:d63ed5a5d1f7d4500a7ac6803cc3310e0f515fdba3cfaf389880ec010715406b size: 3465\n",
      "Finished Step #1\n",
      "PUSH\n",
      "Pushing gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training\n",
      "The push refers to repository [gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training]\n",
      "8325e0a7e446: Preparing\n",
      "b2925d5d803e: Preparing\n",
      "d3c48fa08f3d: Preparing\n",
      "94edcf25cd21: Preparing\n",
      "9f20e5d93682: Preparing\n",
      "d5199d512e13: Preparing\n",
      "d85d8449b661: Preparing\n",
      "8de1bf907713: Preparing\n",
      "98c78de71776: Preparing\n",
      "d5199d512e13: Waiting\n",
      "d85d8449b661: Waiting\n",
      "8de1bf907713: Waiting\n",
      "98c78de71776: Waiting\n",
      "180e2073ce74: Preparing\n",
      "2b4cf8a5bd5e: Preparing\n",
      "180e2073ce74: Waiting\n",
      "80bd043d4663: Preparing\n",
      "30f5cd833236: Preparing\n",
      "7c32e0608151: Preparing\n",
      "2b4cf8a5bd5e: Waiting\n",
      "7cea17427f83: Preparing\n",
      "80bd043d4663: Waiting\n",
      "30f5cd833236: Waiting\n",
      "7c32e0608151: Waiting\n",
      "7cea17427f83: Waiting\n",
      "9f20e5d93682: Layer already exists\n",
      "94edcf25cd21: Layer already exists\n",
      "8325e0a7e446: Layer already exists\n",
      "d3c48fa08f3d: Layer already exists\n",
      "b2925d5d803e: Layer already exists\n",
      "d5199d512e13: Layer already exists\n",
      "8de1bf907713: Layer already exists\n",
      "d85d8449b661: Layer already exists\n",
      "98c78de71776: Layer already exists\n",
      "180e2073ce74: Layer already exists\n",
      "80bd043d4663: Layer already exists\n",
      "2b4cf8a5bd5e: Layer already exists\n",
      "7c32e0608151: Layer already exists\n",
      "30f5cd833236: Layer already exists\n",
      "7cea17427f83: Layer already exists\n",
      "latest: digest: sha256:d63ed5a5d1f7d4500a7ac6803cc3310e0f515fdba3cfaf389880ec010715406b size: 3465\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                    IMAGES                                                            STATUS\n",
      "015a857a-065d-46a3-bb6e-60758ef4144b  2023-11-30T13:10:07+00:00  2M54S     gs://apmena-sandbox-dna-apac-dv_cloudbuild/source/1701349806.283838-e74eb5bf216743c4a8a4e1165a76676e.tgz  gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!gcloud builds submit --config cloudbuild.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20231130131341\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "JOB_NAME=f\"lgb_{TIMESTAMP}\"\n",
    "CUSTOM_CONTAINER_IMAGE_URI=\"gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training:latest\"\n",
    "REGION=\"asia-east2\"\n",
    "\n",
    "print(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "workerPoolSpecs:\n",
    "  machineSpec:\n",
    "    machineType: n1-standard-8\n",
    "  replicaCount: 1\n",
    "  containerSpec:\n",
    "    imageUri: gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://asia-east2-aiplatform.googleapis.com/]\n",
      "CustomJob [projects/1022207864171/locations/asia-east2/customJobs/1238116063573442560] is submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs describe projects/1022207864171/locations/asia-east2/customJobs/1238116063573442560\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai custom-jobs stream-logs projects/1022207864171/locations/asia-east2/customJobs/1238116063573442560\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai custom-jobs create \\\n",
    "  --region={REGION} \\\n",
    "  --display-name={JOB_NAME} \\\n",
    "  --config=config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'apmena-sandbox-dna-apac-dv' \n",
    "gcs_bucket = 'gs://dbx_poc'\n",
    "location = 'asia-east2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=project_id,staging_bucket=gcs_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'machine_spec': {'machine_type': 'n1-standard-8'}, 'replica_count': 1, 'container_spec': {'image_uri': 'gcr.io/apmena-sandbox-dna-apac-dv/ws-vertexai-training:latest'}}]\n"
     ]
    }
   ],
   "source": [
    "worker_pool_specs = [\n",
    "        {\n",
    "            \"machine_spec\": {\n",
    "                \"machine_type\": \"n1-standard-8\"\n",
    "            },\n",
    "            \"replica_count\": 1,\n",
    "            \"container_spec\": {\n",
    "                \"image_uri\": CUSTOM_CONTAINER_IMAGE_URI\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "print(worker_pool_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = vertexai.CustomJob(\n",
    "    display_name=f\"{JOB_NAME}\",\n",
    "    worker_pool_specs=worker_pool_specs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating CustomJob\n"
     ]
    }
   ],
   "source": [
    "job.run(\n",
    "    sync=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing only_model_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile only_model_training.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import db_dtypes\n",
    "\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import joblib\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "\n",
    "client = bigquery.Client() \n",
    "sql = \"\"\"\n",
    "select *\n",
    "from apmena-sandbox-dna-apac-dv.dbx_poc.training_df\n",
    "\"\"\"\n",
    "\n",
    "training_df = client.query(sql).to_dataframe()\n",
    "y = training_df['target_sales']\n",
    "X = training_df.drop(['consumer_id','consumer_code','target_sales','first_order_date','last_order_date','fav_brand_p24m'], axis = 1)\n",
    "X_train, X_t\n",
    "est, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = lgb.LGBMRegressor(objective = 'tweedie', learning_rate=0.1, max_depth=4, reg_lambda=0.5, importance_type = 'gain').fit(X_train, y_train)\n",
    "\n",
    "#write model to gcs bucket\n",
    "import joblib\n",
    "joblib.dump(model, 'lgb_bpp_kr_Luxe231130.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_sample(\n",
    "    experiment_name: str,\n",
    "    experiment_description: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(\n",
    "        experiment=experiment_name,\n",
    "        experiment_description=experiment_description,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m create_experiment_sample(experiment_description\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThis is a test for using experiment\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m experiment_name \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mlgbtest\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m----> 3\u001b[0m project \u001b[39m=\u001b[39m project_id,\n\u001b[1;32m      4\u001b[0m location \u001b[39m=\u001b[39m location)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_id' is not defined"
     ]
    }
   ],
   "source": [
    "create_experiment_sample(experiment_description='This is a test for using experiment',\n",
    "experiment_name = 'lgbtest',\n",
    "project = project_id,\n",
    "location = location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_experiment_run_sample(\n",
    "    experiment_name: str,\n",
    "    run_name: str,\n",
    "    project: str,\n",
    "    location: str,\n",
    "):\n",
    "    aiplatform.init(experiment=experiment_name, project=project, location=location)\n",
    "\n",
    "    aiplatform.start_run(run=run_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/1022207864171/locations/asia-east2/metadataStores/default/contexts/lgbtest-trainingv2 to Experiment: lgbtest\n"
     ]
    }
   ],
   "source": [
    "create_experiment_run_sample('lgbtest','trainingv2',project_id, 'asia-east2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with aiplatform.start_run('training') as my_run:\n",
    "\n",
    "    y = training_df['target_sales']\n",
    "    X = training_df.drop(['consumer_id','consumer_code','target_sales','first_order_date','last_order_date','fav_brand_p24m'], axis = 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #write model to gcs bucket\n",
    "    import joblib\n",
    "    joblib.dump(model, 'lgb_bpp_kr_Luxe231130.pkl')\n",
    "    \n",
    "    #take the trained model hp\n",
    "    my_run.log_params({'learning_rate':0.1, 'max_depth':4, 'reg_lambda':0.5, 'importance_type':'gain'})\n",
    "    model = lgb.LGBMRegressor(objective = 'tweedie', learning_rate=0.1, max_depth=4, reg_lambda=0.5, importance_type = 'gain').fit(X_train, y_train)\n",
    "\n",
    "    aiplatform.log_model(\n",
    "    model=model,\n",
    "    input_example=X.head(5),\n",
    "    display_name='lgb_bpp_kr_Luxe231130',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_job_with_experiment_autologging_sample(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    staging_bucket: str,\n",
    "    display_name: str,\n",
    "    script_path: str,\n",
    "    container_uri: str,\n",
    "    service_account: str,\n",
    "    experiment: str,\n",
    "    experiment_run: str) -> None:\n",
    "    aiplatform.init(project=project, location=location, staging_bucket=staging_bucket, experiment=experiment)\n",
    "\n",
    "    job = aiplatform.CustomJob.from_local_script(\n",
    "        display_name=display_name,\n",
    "        script_path=script_path,\n",
    "        container_uri=container_uri,\n",
    "        enable_autolog=True,\n",
    "    )\n",
    "\n",
    "    job.run(\n",
    "        service_account=service_account,\n",
    "        experiment=experiment,\n",
    "        experiment_run = experiment_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://dbx_poc/aiplatform-2023-11-30-13:46:15.317-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Creating CustomJob\n",
      "CustomJob created. Resource name: projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576\n",
      "To use this CustomJob in another session:\n",
      "custom_job = aiplatform.CustomJob.get('projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576')\n",
      "View Custom Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/asia-east2/training/9101401012962328576?project=1022207864171\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_PENDING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "CustomJob projects/1022207864171/locations/asia-east2/customJobs/9101401012962328576 current state:\n",
      "JobState.JOB_STATE_FAILED\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.10/runpy.py\\\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n  File \\\"/opt/conda/lib/python3.10/runpy.py\\\", line 86, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/root/.local/lib/python3.10/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 7, in <module>\\n    from all_imports import *\\nModuleNotFoundError: No module named \\'all_imports\\'\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1022207864171&resource=ml_job%2Fjob_id%2F9101401012962328576&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%229101401012962328576%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_custom_job_with_experiment_autologging_sample(\n\u001b[1;32m      2\u001b[0m     project_id,\n\u001b[1;32m      3\u001b[0m     location,\n\u001b[1;32m      4\u001b[0m     staging_bucket\u001b[39m=\u001b[39;49mgcs_bucket,\n\u001b[1;32m      5\u001b[0m     display_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtraining job of lgb model\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     script_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./ws_verAI_training.py\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     container_uri\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39masia-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     service_account \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m1022207864171-compute@developer.gserviceaccount.com\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     experiment \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mlgbtest\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     experiment_run \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrainingv2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[29], line 20\u001b[0m, in \u001b[0;36mcreate_custom_job_with_experiment_autologging_sample\u001b[0;34m(project, location, staging_bucket, display_name, script_path, container_uri, service_account, experiment, experiment_run)\u001b[0m\n\u001b[1;32m     11\u001b[0m aiplatform\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39mproject, location\u001b[39m=\u001b[39mlocation, staging_bucket\u001b[39m=\u001b[39mstaging_bucket, experiment\u001b[39m=\u001b[39mexperiment)\n\u001b[1;32m     13\u001b[0m job \u001b[39m=\u001b[39m aiplatform\u001b[39m.\u001b[39mCustomJob\u001b[39m.\u001b[39mfrom_local_script(\n\u001b[1;32m     14\u001b[0m     display_name\u001b[39m=\u001b[39mdisplay_name,\n\u001b[1;32m     15\u001b[0m     script_path\u001b[39m=\u001b[39mscript_path,\n\u001b[1;32m     16\u001b[0m     container_uri\u001b[39m=\u001b[39mcontainer_uri,\n\u001b[1;32m     17\u001b[0m     enable_autolog\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m job\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     21\u001b[0m     service_account\u001b[39m=\u001b[39;49mservice_account,\n\u001b[1;32m     22\u001b[0m     experiment\u001b[39m=\u001b[39;49mexperiment,\n\u001b[1;32m     23\u001b[0m     experiment_run \u001b[39m=\u001b[39;49m experiment_run)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:1714\u001b[0m, in \u001b[0;36mCustomJob.run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   1711\u001b[0m network \u001b[39m=\u001b[39m network \u001b[39mor\u001b[39;00m initializer\u001b[39m.\u001b[39mglobal_config\u001b[39m.\u001b[39mnetwork\n\u001b[1;32m   1712\u001b[0m service_account \u001b[39m=\u001b[39m service_account \u001b[39mor\u001b[39;00m initializer\u001b[39m.\u001b[39mglobal_config\u001b[39m.\u001b[39mservice_account\n\u001b[0;32m-> 1714\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\n\u001b[1;32m   1715\u001b[0m     service_account\u001b[39m=\u001b[39;49mservice_account,\n\u001b[1;32m   1716\u001b[0m     network\u001b[39m=\u001b[39;49mnetwork,\n\u001b[1;32m   1717\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1718\u001b[0m     restart_job_on_worker_restart\u001b[39m=\u001b[39;49mrestart_job_on_worker_restart,\n\u001b[1;32m   1719\u001b[0m     enable_web_access\u001b[39m=\u001b[39;49menable_web_access,\n\u001b[1;32m   1720\u001b[0m     experiment\u001b[39m=\u001b[39;49mexperiment,\n\u001b[1;32m   1721\u001b[0m     experiment_run\u001b[39m=\u001b[39;49mexperiment_run,\n\u001b[1;32m   1722\u001b[0m     tensorboard\u001b[39m=\u001b[39;49mtensorboard,\n\u001b[1;32m   1723\u001b[0m     sync\u001b[39m=\u001b[39;49msync,\n\u001b[1;32m   1724\u001b[0m     create_request_timeout\u001b[39m=\u001b[39;49mcreate_request_timeout,\n\u001b[1;32m   1725\u001b[0m     disable_retries\u001b[39m=\u001b[39;49mdisable_retries,\n\u001b[1;32m   1726\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:817\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    816\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[39m.\u001b[39mwait(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 817\u001b[0m     \u001b[39mreturn\u001b[39;00m method(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    819\u001b[0m \u001b[39m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    820\u001b[0m internal_callbacks \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:1815\u001b[0m, in \u001b[0;36mCustomJob._run\u001b[0;34m(self, service_account, network, timeout, restart_job_on_worker_restart, enable_web_access, experiment, experiment_run, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   1743\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Helper method to ensure network synchronization and to run the configured CustomJob.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \n\u001b[1;32m   1745\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1800\u001b[0m \u001b[39m        `restart_job_on_worker_restart` to False.\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1802\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubmit(\n\u001b[1;32m   1803\u001b[0m     service_account\u001b[39m=\u001b[39mservice_account,\n\u001b[1;32m   1804\u001b[0m     network\u001b[39m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1812\u001b[0m     disable_retries\u001b[39m=\u001b[39mdisable_retries,\n\u001b[1;32m   1813\u001b[0m )\n\u001b[0;32m-> 1815\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_block_until_complete()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/jobs.py:1111\u001b[0m, in \u001b[0;36m_RunnableJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[39m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[39m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39mstate \u001b[39min\u001b[39;00m _JOB_ERROR_STATES:\n\u001b[0;32m-> 1111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJob failed with:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gca_resource\u001b[39m.\u001b[39merror)\n\u001b[1;32m   1112\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1113\u001b[0m     _LOGGER\u001b[39m.\u001b[39mlog_action_completed_against_resource(\u001b[39m\"\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcompleted\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.10/runpy.py\\\", line 196, in _run_module_as_main\\n    return _run_code(code, main_globals, None,\\n  File \\\"/opt/conda/lib/python3.10/runpy.py\\\", line 86, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/root/.local/lib/python3.10/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 7, in <module>\\n    from all_imports import *\\nModuleNotFoundError: No module named \\'all_imports\\'\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=1022207864171&resource=ml_job%2Fjob_id%2F9101401012962328576&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%229101401012962328576%22\"\n"
     ]
    }
   ],
   "source": [
    "create_custom_job_with_experiment_autologging_sample(\n",
    "    project_id,\n",
    "    location,\n",
    "    staging_bucket=gcs_bucket,\n",
    "    display_name='training job of lgb model',\n",
    "    script_path='./ws_verAI_training.py',\n",
    "    container_uri= 'asia-docker.pkg.dev/vertex-ai/training/sklearn-cpu.1-0:latest',\n",
    "    service_account = '1022207864171-compute@developer.gserviceaccount.com',\n",
    "    experiment = 'lgbtest',\n",
    "    experiment_run = 'trainingv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
